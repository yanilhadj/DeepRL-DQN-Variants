Dans le cadre de ce projet, nous avons mis en œuvre des algorithmes d'apprentissage par renforcement profond pour résoudre divers problèmes proposés par Gymnasium. Initialement, notre choix s'est porté sur un environnement complexe, tel que le jeu de ping-pong (Pong-v5). Ce dernier est un environnement multidimensionnel avec des observations de dimensions (210, 160, 3) et un espace d'actions discret composé de six mouvements. Cependant, ses spécificités ont entraîné une consommation excessive de ressources matérielles, nous obligeant à nous tourner vers des environnements plus simples et analytiquement accessibles. Ce rapport présente notre démarche, les solutions adoptées et les résultats obtenus.

Il est important de noter que face à cette limitation matérielle pour le jeu de ping-pong, nous avons envisagé de segmenter les épisodes en sauvegardant périodiquement le modèle afin de poursuivre l'apprentissage ultérieurement. Cependant, cette approche ne permettait pas de vérifier efficacement si l’agent apprenait réellement, en raison d’une progression trop lente de sa performance.

Nous avons donc opté pour un environnement unidimensionnel, proposé dans la famille Classical Control. Ces environnements unidimensionnels présentent l’avantage d’être plus facilement évaluables, car l’atteinte de l’objectif peut être directement observée.

Nous avons fait le choix de tester nos algorithmes sur l’environnement MountainCar-v0, un environnement où une voiture doit accumuler de l’énergie cinétique pour atteindre le sommet d'une colline. Cet environnement nous est familier car évoqué dans le cours.

Nous avons opté pour l’algorithme du DQN afin de répondre aux spécificités de l’environnement étudié. En effet, le Deep Q-Network (DQN) a été choisi car il s'agit d'une méthode basée sur les valeurs (value-based), particulièrement efficace pour des espaces d’états discrets tels que ceux rencontrés dans MountainCar-v0. L’objectif serait ici de comparer quatre différentes variantes de DQN sur un même problème afin de comparer leur efficacité.

Nous avons jugé intéressant d’implémenter ces algorithmes, car nous n’en avons pas eu l’occasion dans le cadre du cours. Lors des séances de travaux pratiques, nous avons uniquement exploré les méthodes Dynamic Programming (DP), Monte Carlo (MC) et Temporal Difference (TD). Ce projet nous a donc offert l'opportunité d'explorer de nouvelles approches et de découvrir différentes variantes de DQN, telles que DQN avec Experience Replay, Prioritized Experience Replay et Double DQN, afin d’analyser leur impact sur l’apprentissage et la performance des agents.